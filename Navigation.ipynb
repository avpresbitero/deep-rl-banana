{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRLBanana : Navigation Project\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook contains the implementation for the Udacity's Deep Reinforcement Learning Navigation project. It uses the Unity ML Agents environment and runs a Deep Q-Learning model on the back-end to train the agent into collecting yellow bananas! \n",
    "\n",
    "Installation instructions as well as requirements have been specied in the README file of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# % matplotlib inline\n",
    "\n",
    "# import the custom RL agent\n",
    "from dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Main DQN Loop\n",
    "\n",
    "The dqn function is responsible for executing the core Deep Q Learning loop. Once a score of 13 or more has been achieved, the model weights are saved to a checkpoint file and the training is ended. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                                           # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)                     # last 100 scores\n",
    "    eps = eps_start                                       # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]           # get states\n",
    "        score = 0                                         # set initial score\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                           # get action value\n",
    "            env_info = env.step(action.astype(int))[brain_name]      \n",
    "            next_state = env_info.vector_observations[0]             # get next state\n",
    "            reward = env_info.rewards[0]                             # get reward\n",
    "            done = env_info.local_done[0]                            # get value if done (boolean)\n",
    "            agent.step(state, action, reward, next_state, done)      # moves agent one step\n",
    "            state = next_state                                       # sets state to next state\n",
    "            score += reward                                          # appends the reward to the total score\n",
    "            if done:                                                 # check if done\n",
    "                break                                                # breaks loop of episode\n",
    "        scores_window.append(score)                       # save most recent score\n",
    "        scores.append(score)                              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps)                 # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:                          # tracks mean of consecutive 100 episodes\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the Unity environment\n",
    "\n",
    "Using the Banana.app environment, we construct the unity \"brain\" and the gather some basic meta data about the environment such as state and action spaces so that we can configure the DNN model initial and final layer sizes appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(\"Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the agent\n",
    "\n",
    "Kick off the training and store the score history for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "Episode 100\tAverage Score: 0.89\n",
      "Episode 200\tAverage Score: 3.87\n",
      "Episode 300\tAverage Score: 7.99\n",
      "Episode 400\tAverage Score: 9.27\n",
      "Episode 500\tAverage Score: 11.66\n",
      "Episode 600\tAverage Score: 13.80\n",
      "Episode 700\tAverage Score: 13.94\n",
      "Episode 800\tAverage Score: 14.20\n",
      "Episode 900\tAverage Score: 13.67\n",
      "Episode 1000\tAverage Score: 14.53\n",
      "Episode 1100\tAverage Score: 15.63\n",
      "Episode 1187\tAverage Score: 16.13"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "print(agent.qnetwork_local)\n",
    "\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the results\n",
    "\n",
    "The raw scores chart shows the results of the individual espisodes. You will not it is very noisy. By applying a low pass filter, in this case a moving average, over the scores we can see clearly see the improvement in the agent over the episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate moving average\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "ls = 16\n",
    "ts = 18\n",
    "\n",
    "# plot the scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "ax1.plot(np.arange(len(scores)), scores)\n",
    "ax1.set_ylabel('Score', fontsize=ls)\n",
    "ax1.set_xlabel('Episode #', fontsize=ls)\n",
    "ax1.set_title(\"Scores\", fontsize=ts)\n",
    "\n",
    "avg_score = moving_average(scores, 100)\n",
    "ax2.axhline(y=13., xmin=0.0, xmax=1.0, color='r', linestyle='--', linewidth=0.7, alpha=0.9)\n",
    "ax2.plot(np.arange(len(avg_score)), avg_score)\n",
    "ax2.set_ylabel('Average Score', fontsize=ls)\n",
    "ax2.set_xlabel('Episode #', fontsize=ls)\n",
    "ax2.set_title(\"Moving average scores\", fontsize=ts)\n",
    "\n",
    "fig.savefig('dqn.jpg', format='jpg')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Play the trained agent\n",
    "\n",
    "Now that we have a trained agent, we can reset the environment into non-training and then load the best weights for the DNN and run the agent through the Unity environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to get the state space \n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "\n",
    "# create the agent \n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "#  run the agent ten times\n",
    "for i in range(10):\n",
    "    score = 0\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    for j in range(1000):\n",
    "        action = agent.act(state)                      # select an action\n",
    "        env_info = env.step(action.astype(int))[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "    print('\\rScore: {:.2f}'.format(score))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Close the environment\n",
    "\n",
    "Once we are done, close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Recommendations\n",
    "\n",
    "Solving the task only required a shallow architecture as a task as simple as this would not require more complicated algorithms. However, Dueling DQN or other enhancements are still worth exploring as I have noticed that increasing the episodes did not do much to the learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da27380075e1ba67d4ee2df9ef86f2337c93492f93a82bdb709e3a850cd515a8"
  },
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
